All Proc objects have the property of closure, which allows the code within the code block to access the local variables of the caller's scope. While that sounds highly academic, it has a very practical benefit that you're probably already aware of. The closure property of Proc objects is what makes the following snippet of code possible.

    class Vector
      def initialize(data)
        @data = data 
      end

      def *(num)
        @data.map { |e| e * num }
      end
    end

    >> Vector.new([1,2,3]) * 7
    => [7, 14, 21]

In this example, when we call @data.map and pass it a code block to execute, we have no trouble accessing the num variable. However, this local variable is not defined within the block's local scope, it's defined within the scope of the caller (the Vector#*) method. To show that these are truly two different scopes, here are a few small examples about the relationship between the Proc object's code and it's caller.

    def proc_can_see_locals_of_caller
      y = 10
      lambda { p defined?(y) }.call
    end

    def proc_can_modify_locals_of_caller
      y = 10
      lambda { y = 20 }.call
      p y
    end

    def proc_destroys_block_local_vars_on_exit
      lambda { y = 10 }.call
      p defined?(y)
    end

    proc_can_see_locals_of_caller            #=> "local-variable"
    proc_can_modify_locals_of_caller         #=> 20
    proc_destroys_block_local_vars_on_exit   #=> nil

Our first example demonstrates that a Proc object's code can access the local variables of its caller, which is exactly what is going on in our Vector example. The second example is an answer to a question which arises naturally from the first example, which is whether or not the Proc object's code can modify the contents of the caller's local variables. The third example simply verifies that once the Proc has been called, any variable set up within its own code block are wiped out and are not visible from the caller's scope.

## Closures make memory management complicated

While it takes some getting used to, the behaviors provided by the closure property in Proc objects are relatively easy to understand and have many practical benefits. However, they do give rise to a complex behavior that sometimes leads to surprising results. Check out the example below for a bit of a head trip.

    def new_counter
      x = 0
      lambda { x += 1 }
    end

    counter_a = new_counter
    counter_a.call
    counter_a.call

    p counter_a.call #=> 3

    counter_b = new_counter
    p counter_b.call #=> 1

    p counter_a.call #=> 4

In the code above, we see the two Proc objects returned by the new_counter() method are referencing two different locations in memory. This is a bit confusing, because we can usually count on methods to destroy the local variables we used within them once they wrap up with whatever they are doing. But since the purpose of a Proc object is in part to be able to delay the execution of code, it's impossible for the new_counter() method to do this cleanup task for us. And so here's what happens: counter_a gets a reference to the local variable x that was set up the first time we called new_counter(), and counter_b gets a reference to a different local variable x that was set up the second time we called new_counter().

If used correctly, this behavior can be a feature. It's not one that you or I would use day to day, but it is interesting if you are doing some heavy functional programming, because you can use this approach to maintain state in a purely functional way. However, in most ordinary use cases, it's much more likely that this behavior is going to cause a memory leak than it is to do anything helpful for you, since it leads to lots of seemingly throwaway data stored in local variables getting dangling references that prevent that data from being garbage collected.

## A more common way of introducing memory leaks via closures

To make matters worse, the problem we just discussed isn't the only way that you can cause leaks with Proc objects. Every Proc object also creates a reference to the caller itself, which is an even harder to notice leak. Let's take a look at an example of how that can come back to bite you.

Suppose we have a configurable logger module and we want to record a message to the logs each time a new User object is created. If we were going for something simple and straightforward, we might end up with code similar to what you see below.

    module Logger
      extend self

      attr_accessor :output

      def log(msg)
        output << "#{msg}\n"
      end
    end

    class User
      def initialize(id)
        @id = id
        Logger.log("Created User with ID #{id}")
      end
    end

But if we wanted to be a bit more fancy, we could consider building a logger that delayed the writing of the logs until you explicitly asked for them to be written. We could use Proc objects for lazy evaluation, giving us a potential speed boost whenever we didn't actually need to view our logs.

  module LazyLogger
    extend self

    attr_accessor :output

    def log(&block)
      @log_actions ||= []
      @log_actions << block
    end

    def flush
      @log_actions.each { |e| e.call(output) }
    end
  end

  class User
    def initialize(id)
      @id = id
      LazyLogger.log { |out| out << "Created User with ID #{id}" }
    end
  end

While implementing this strategy is somewhat straightforward, it leads to a memory leak. The leak can be verified via the simple script below, which shows that 1000 users still exist in the system even though the objects were created as throwaway objects:

    LazyLogger.output = ""
    1000.times { |i| User.new(i) }

    GC.start
    p ObjectSpace.each_object(User).count #=> 1000

If instead we use our more vanilla code that does not use Proc objects, we see that for the most part*, the GC has done its job.

    Logger.output = ""
    1000.times { |i| User.new(i) }

    GC.start
    
    # (*): I expected below to be 0, but GC clearly ran. Weird.
    p ObjectSpace.each_object(User).count #=> 1 

The reason why our LazyLogger leaks is that when LazyLogger.log is called with a block from within User#initialize, a new Proc object is created that holds a reference to that user object. That Proc object ends up getting stored in the @log_actions array in LazyLogger module, and needs to be kept alive at least until LazyLogger.flush is called in order for everything to work as expected. That means that our User objects that we expected to get thrown away still have live references to them, and so don't end up getting garbage collected.

These kinds of problems can be very easy to run into, and very hard to work around. In fact, I've have been having trouble figuring out a way to preserve the LazyLogger behavior in a way that'd plug the leak or at mitigate it somewhat. My core assumption was that if I added a @log_actions = [] line to LazyLogger.flush, that the dangling references to the Users would be cleaned up each time that method was called, because the Proc objects themselves would be GCed. No matter what I tried, I was unable to get this to work, which tells me that I've got a leak somewhere else in my program that's not the one I was intentionally creating.

This means that many Ruby applications and libraries have memory leaks in them. Even fairly experienced developers (myself included) don't necessarily design with these issues in mind, and those that do need to use a variety of awkward techniques to overcome this problem. One possible solution is to use Method objects in place of Proc objects wherever the closure properties are not required. Another is to create Proc objects in a different context to avoid accidental references to objects that you want GCed. I recently played with some code that used the latter approach in order to make use of ObjectSpace.define_finalizer. While that's a bit of an obscure topic, it's a good example of what we've just been talking about, so I'd recommend checking it out.

http://www.mikeperham.com/2010/02/24/the-trouble-with-ruby-finalizers/

I don't want to give much more advice on handling memory management, because it's not an area that I'm particularly strong at. I would however, be happy to at least think through any questions you might have about how closures can lead to memory leaks, and how to avoid those scenarios where possible. I would also welcome any corrections to what I've said here, if you find that I've made a mistake anywhere.

## Some other points to consider about Ruby closures

This article wasn't especially long, but the material is quite dense and I don't want to push my luck with how much effort you will be willing to put into soaking up all this information. That said, I have three simple homework assignments related to this topic that are worth exploring for yourself if you haven't considered them before. It should be easily enough to verify the right answers to these problems by simply writing a script to prove your assumptions. Try the do-it-yourself approach before looking the answers up or asking a friend, as you'll learn more that way.

1) A benefit of Ruby's define_method() function is that you can use the closure property to access the enclosing scope, allowing for code such as the example below. e 
     
  def my_attr_reader(*attributes)
    attributes.each do |a|
      define_method(a) { instance_variable_get("@#{a}") }
    end
  end

The alternative approach to dynamically defining methods is a bit more kludgey, involving class_eval and a dynamically generated string. Despite its many downsides, one benefit of using this approach is that it does not incur the performance penalty that define_method() does, as methods that are generated in this way do not need to consider the caller's context.

  def my_attr_reader(*attributes)
    attributes.each do |a|
      class_eval %{ def #{a}; @#{a}; end }
    end
  end

Ignoring the particular example and considering the different techniques in general, what are the tradeoffs between these two approaches? Compare the method call performance for each approach as well as statically defined Ruby methods. If you find a performance difference between the two, consider whether it's significant enough to effect your decisions about using one technique over the other in various contexts.

2) First, examine the code below without running it and think about what you expect it to output. Then run it on Ruby 1.8.7 and Ruby 1.9.2 and then think about why the results differ from each other (and possibly from your own!)

    x = 10
    (1..3).each { |x| x }
    p x

Some considered the Ruby 1.8.7 behavior to be a bug, while others thought it was a feature. Officially it was just a strange behavior that was changed in Ruby 1.9.2 to be less surprising (or surprising in a different way). Can you find a valid use case for the Ruby 1.8.7 behavior? Aside from helping avoid subtle bugs, can you find other reasons why the Ruby 1.9.2 behavior is desireable? 

3) In Ruby 1.9.2, it is possible for methods defined using define_method to accept code blocks. This was not possible in Ruby 1.8.7, but there was a class_eval() based workaround. Try to implement that workaround if you don't already know what it looks like.

If you attempt these exercises, feel free to share your results on the mailing list, along with any other questions or comments you have about this article.
